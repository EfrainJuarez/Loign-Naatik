# -*- coding: utf-8 -*-
"""Prueba_XGB.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1E2wVI8nSHTFI0yePuzj01BgDYqIbr0iy
"""

import pandas as pd
import seaborn as sns
import numpy as np
import sklearn
from sklearn.preprocessing import LabelEncoder
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.model_selection import GridSearchCV
from sklearn import metrics
from sklearn.metrics import accuracy_score
from sklearn.metrics import ConfusionMatrixDisplay
import xgboost

dataset = pd.read_csv('d:/TEC/Practicas Naatik/IA//dataTelco.csv')

util_dataset=dataset=dataset.drop('customerID', axis=1)

for column in util_dataset.columns:
   if util_dataset[column].dtype == np.number:
      continue
   util_dataset[column] = LabelEncoder().fit_transform(util_dataset[column])

X = util_dataset.loc[:,util_dataset.columns!='Churn']
y = util_dataset.loc[:,'Churn']

X = StandardScaler().fit_transform(X)

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 0)

xgb = xgboost.XGBClassifier()

parameters = {'nthreads': [1],
             'objective': ['binary:logistic'],
             'learning_rate': [0.1],
             'n_estimators': [200]}

fit_params = {'early_stopping_rounds': 10,
             'eval_metric': 'logloss',
             'eval_set':[(X_test, y_test)]}

clf = GridSearchCV(xgb, parameters, cv=3, scoring = 'accuracy')

clf.fit(X_train, y_train, **fit_params)

#visualizar aciertos  (descartable)
ConfusionMatrixDisplay.from_estimator(estimator=clf, X=X_test, y=y_test, values_format='.0f')

#calcular la exactitud  (descartable)
best_xgb = clf.best_estimator_

y_preds = best_xgb.predict(X_train)

exactitud = accuracy_score(y_train, y_preds)
exactitud

ch = clf.predict(X_test)

#columna = ["Pred_Churn"]

#ch = pd.DataFrame(ch, index=y_preds.index.values, columns=columna)

#predic = pd.DataFrame({'real': y_test, 'prediccion': y_preds})
#predic.head(30)

ch

#Guardamos el modelo en un archivo con permisos de escritura
XGBoost_modelo = open('xgboost Churn.sav', 'wb')

from pickle import dump
from pickle import load
#con la funcion dump guardamos el modelo en el archivo que creamos
dump(clf, XGBoost_modelo)

#abrimos el nuevo dataframe con los datos a predecir
clasificar_nuevos = pd.read_csv('./entrada_convertida_og_ID_no churn.csv', engine='python', index_col=0)

#cargamos nuestro modelo con formato de lectura
modelo_cargado = load(open('xgboost Churn.sav', 'rb'))

prediccion = modelo_cargado.predict(clasificar_nuevos)

columna = ["Pred_Churn"]

prediccion = pd.DataFrame(prediccion, index=clasificar_nuevos.index.values, columns=columna)

prediccion.head(10)

predichos = pd.concat([clasificar_nuevos, prediccion], axis=1)
predichos.head(10)

#guardamos el los resultados en un archivo .csv
predichos.to.cdv('d:/TEC/Practicas Naatik/IA/prediccion_xgb.csv')